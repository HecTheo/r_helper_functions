print(t2-t1)
return(final_df)
}
sent_pipe(txt_input_col = data$text
, txt_id_col = data$text_id
, dimension = 'concreteness'
, stemming = F
, transposing = T)
a = sent_pipe(txt_input_col = data$text
, txt_id_col = data$text_id
, dimension = 'concreteness'
, stemming = F
, transposing = T)
a = sent_pipe(txt_input_col = data$text
, txt_id_col = data$text_id
, dimension = 'concreteness'
, stemming = F
, transposing = F)
plot(1:100
, a$text7
, type ="l"
, col = "red")
plot(1:100
, a$text7
, type ="h"
, col = "red")
b = sent_pipe(txt_input_col = data$text
, txt_id_col = data$text_id
, dimension = 'concreteness'
, stemming = T
, transposing = F)
plot(1:100
, a$text7
, type ="l"
, col = "red")
plot(1:100
, b$text7
, type ="l"
, col = "red")
plot(1:100
, a$text7
, type ="l"
, col = "red")
lines(1:100
, b$text7
, type ="l"
, col = "blue")
c = sent_pipe(txt_input_col = data$text
, txt_id_col = data$text_id
, dimension = 'sentiment'
, stemming = F
, transposing = F)
d = sent_pipe(txt_input_col = data$text
, txt_id_col = data$text_id
, dimension = 'sentiment'
, stemming = T
, transposing = F)
plot(1:100
, c$text7
, type ="l"
, col = "red")
lines(1:100
,  d$text7
, type ="l"
, col = "blue")
plot(1:100
, c$text8
, type ="l"
, col = "red")
lines(1:100
,  d$text8
, type ="l"
, col = "blue")
data = data.frame('text' = character()
, 'text_id' = character())
data
concr.dict_stemmed
data = data.frame('text' = character()
, 'text_id' = character())
data$text[1] = 'this is a super, great positive sentence and I just love doing this'
data$text[2] = 'and now a bad, bad, and ugly example which I really hate'
data$text_id[1] = 'text1'
data$text_id[2] = 'text2'
data = data.frame('text' = character()
, 'text_id' = character())
data$text[1] = 'this is a super, great positive sentence and I just love doing this'
data$text[1]
character(2)
data = data.frame('text' = character(2)
, 'text_id' = character(2))
data$text[1] = 'this is a super, great positive sentence and I just love doing this'
data$text[2] = 'and now a bad, bad, and ugly example which I really hate'
data$text_id[1] = 'text1'
data$text_id[2] = 'text2'
data$text
data = data.frame('text' = character(2)
, 'text_id' = character(2))
data$text = c('this is a super, great positive sentence and I just love doing this'
, 'and now a bad, bad, and ugly example which I really hate')
data
data$text_id = c('text1', 'text2')
data
my_sentiment_analysis = sent_pipe(txt_input_col = data$text
, txt_id_col = data$text_id
, dimension = 'sentiment'
, stemming = F
, transposing = F)
plot(1:100
, my_sentiment_analysis$text1
, type ="l"
, col = "red")
plot(1:100
, my_sentiment_analysis$text1
, type ="h"
, col = "red")
my_concreteness_analysis = sent_pipe(txt_input_col = data$text
, txt_id_col = data$text_id
, dimension = 'concreteness'
, stemming = F
, transposing = F)
plot(1:100
, my_cocnreteness_analysis$text1
, type ="h"
, col = "red")
plot(1:100
, my_concreteness_analysis$text1
, type ="h"
, col = "red")
source('./sent_pipe_syuzhet_plus.R)
)
###########################################################
### TERR CLAIMS PROJECT
### ANALYSIS
### B Kleinberg, I van der Vegt, M Zaneva, B Verschuere
###########################################################
#clear wd
rm(list = ls())
#deps
setwd("/Users/bennettkleinberg/GitHub/r_helper_functions/")
source('./txt_df_from_dir.R')
source('./spacy_ner_r.R')
source('./spacy_pos_r.R')
source('./linguistic_category_model.R')
source('./calculate_concreteness.R')
source('./get_single_readability.R')
source('./sent_pipe_syuzhet_plus.R')
source('./toNumeric.R')
source('./ds_between_CI.R')
source('./dz_within_CI.R')
source('/Users/bennettkleinberg/GitHub/r_helper_functions/cohensf.R')
###########################################################
### TERR CLAIMS PROJECT
### ANALYSIS
### B Kleinberg, I van der Vegt, M Zaneva, B Verschuere
###########################################################
#clear wd
rm(list = ls())
#deps
setwd("/Users/bennettkleinberg/GitHub/r_helper_functions/")
source('./txt_df_from_dir.R')
source('./spacy_ner_r.R')
source('./spacy_pos_r.R')
source('./linguistic_category_model.R')
source('./calculate_concreteness.R')
source('./get_single_readability.R')
source('./sent_pipe_syuzhet_plus.R')
source('./toNumeric.R')
source('./ds_between_CI.R')
source('./dz_within_CI.R')
#packages
library(tidyr)
library(ggplot2)
library(Hmisc)
library(caret)
library(e1071)
library(pROC)
library(ez)
#library(quanteda)
library(MLmetrics)
setwd('/Users/bennettkleinberg/Documents/Research/CBDMI_Schiphol/terr_claims/data')
load('data_terr_claims.RData')
#merge with country annotation
country = read.csv('terr_claims_with_region.csv')
country = country[, c(3, 4)]
names(country) = c('statement_id', 'country')
country = na.omit(country)
country$country = as.factor(country$country)
levels(country$country) = c('ME', 'WEST', 'ASIA', 'AFRICA')
country$Filename = paste(as.character(country$statement_id), ".txt", sep="")
#merge with dates
dates = read.csv('terr_claims_with_date.csv')
dates = dates[, c(2, 11)]
dates$publication_date = as.Date(as.character(dates$publication_date)
, format = "%d/%m/%Y")
#####PART 2#####
#####Preparation#####
##regex for magazine, issue number
data$magazine = as.factor(substr(as.character(data$Filename), 1, 1))
data$issue = str_extract(as.character(data$Filename), pattern = '[[:digit:]]{1,2}')
data$statement_id = str_extract(gsub(pattern = '[[:upper:]]_[[:digit:]]{1,2}', "", as.character(data$Filename)), pattern = '[[:digit:]]{1,2}')
levels(data$magazine) = c('dabiq', 'rumiyah')
#merge
data = merge(data, country, by='Filename')
data = merge(data, dates, by='Filename')
#descriptive sample statistics
tapply(data$nwords, data$magazine, mean)
tapply(data$nwords, data$magazine, sd)
mean(data$nwords)
sd(data$nwords)
prop.table(table(data$country, data$magazine), 2)*100
prop.table(table(data$country))*100
table(data$magazine)
##set coding criterion score for veracity
minimum_source_agreement = 1
data$veracity = ifelse(data$isis_sum >= minimum_source_agreement, 1, 0)
prop.table(table(data$veracity))*100
prop.table(table(data$veracity, data$magazine), 2)*100
#set vars
data$how = data$pos_adj_rel + data$pos_adv_rel
data$why = data$cause.x/100
data$negativity = data$negemo.x - data$posemo.x
data$inf_language = data$pos_noun_rel + data$pos_adj_rel + data$prep.x/100 + data$pos_sconj_rel + data$pos_cconj_rel
data$imag_language = data$pos_verb_rel + data$pos_adv_rel + data$pos_pron_rel
data$person_index_weighted = (1*data$ppron.x + 2*data$social.x + 3*data$ner_person_nn)/(data$ppron.x + data$social.x + data$ner_person_nn)
data$perspective = (data$i.x + data$we.x) - (data$you.x + data$they.x)
data$first_ppron = data$i.x + data$we.x
data$third_ppron = data$shehe.x + data$they.x
data$personal_concerns = data$work.x + data$leisure.x + data$home.x + data$money.x + data$relig.x + data$death.x
##set id for merge later
data$text_id = paste('text', row.names(data), sep="")
data_min = data[, c('veracity', 'text_id')]
a = sent_pipe(txt_input_col = data$text[1:20]
, txt_id_col = data$text_id[1:20]
, dimension = 'sentiment'
, stemming = F
, transposing = F)
#deps
setwd("/Users/bennettkleinberg/GitHub/r_helper_functions/")
a = sent_pipe(txt_input_col = data$text[1:20]
, txt_id_col = data$text_id[1:20]
, dimension = 'sentiment'
, stemming = F
, transposing = F)
a
require(syuzhet)
require(tm)
#require(qdap)
#needs wd with helper functions
#easiest is to use the repo on: https://github.com/ben-aaron188/r_helper_functions
#DO NOT RUN
# syuzhet.dict = get_sentiment_dictionary()
# syuzhet.dict_corpus_file = Corpus(VectorSource(syuzhet.dict$word))
# syuzhet.dict_stemmed = tm_map(syuzhet.dict_corpus_file, stemDocument, language = 'en')
# syuzhet.dict_stemmed_df = as.data.frame(as.matrix(syuzhet.dict_stemmed$content))
# names(syuzhet.dict_stemmed_df) = 'word'
# syuzhet.dict = cbind(syuzhet.dict, syuzhet.dict_stemmed_df$word)
# names(syuzhet.dict)[3] = 'word_stemmed'
# syuzhet.dict = syuzhet.dict[, c(3,2)]
# names(syuzhet.dict)[1] = 'word'
# syuzhet.dict = aggregate(value ~ word
#                          , data=syuzhet.dict
#                          , FUN=mean)
# save(syuzhet.dict
#      , file = './syuzhet_dep/syuzhet_dict_stemmed.RData')
#
# concr = fread('./concreteness/Concreteness_ratings_Brysbaert_et_al_BRM.txt'
#               , header=T)
# concr = as.data.frame(concr)
# names(concr) = tolower(names(concr))
# concr_corpus_file = Corpus(VectorSource(concr$word))
# concr_stemmed = tm_map(concr_corpus_file, stemDocument, language = 'en')
# concr_stemmed_df = as.data.frame(as.matrix(concr_stemmed$content))
# names(concr_stemmed_df) = 'word'
# concr = cbind(concr, concr_stemmed_df$word)
# names(concr)[10] = 'word_stemmed'
# concr.dict_stemmed = concr[, c(10, 3)]
# names(concr.dict_stemmed) = c('word', 'value')
# concr.dict_stemmed = aggregate(value ~ word
#                          , data=concr.dict_stemmed
#                          , FUN=mean)
# concr.dict_nonstemmed = concr[, c(1, 3)]
# names(concr.dict_nonstemmed) = c('word', 'value')
# concr.dict_nonstemmed = aggregate(value ~ word
#                                , data=concr.dict_nonstemmed
#                                , FUN=mean)
#
# save(concr.dict_stemmed
#      , concr.dict_nonstemmed
#      , file = './syuzhet_dep/brysbaert_dict_two_versions.RData')
#END DO NOT RUN
get_narrative_dim = function(txt_input_col
, txt_id_col
, dimension
, stemming = T
, bins = 100
, transposing = F){
currentwd = getwd()
t1 = Sys.time()
if(dimension == 'concreteness'){
load('./syuzhet_dep/brysbaert_dict_two_versions.RData')
if(stemming == T){
concr_lexicon = concr.dict_stemmed
} else if(stemming == F){
concr_lexicon = concr.dict_nonstemmed
}
} else if(dimension == 'sentiment'){
load('./syuzhet_dep/syuzhet_dict_stemmed.RData')
if(stemming == T){
sent_lexicon = syuzhet.dict
} else if(stemming == F){
sent_lexicon = get_sentiment_dictionary()
}
}
if(stemming == T){
txt_col = sapply(txt_input_col, function(x){
tm_vec_col = Corpus(VectorSource(x))
tm_vec_col = tm::tm_map(tm_vec_col, content_transformer(replace_contraction))
tm_vec_col = tm_map(tm_vec_col, content_transformer(replace_number))
tm_vec_col = tm_map(tm_vec_col, content_transformer(replace_abbreviation))
tm_vec_col = tm_map(tm_vec_col, content_transformer(tolower))
tm_vec_col = tm_map(tm_vec_col, removeWords, tm::stopwords("en"))
tm_vec_col = tm_map(tm_vec_col, stemDocument, language = 'en')
as.character(as.matrix(tm_vec_col$content))
})
} else if(stemming == F){
txt_col = txt_input_col
}
empty_matrix = matrix(data = 0
, nrow = bins
, ncol = length(txt_col)
)
for(i in 1:length(txt_col)){
print(paste('---> performing sentiment extraction on text: ', txt_id_col[i], sep=""))
text.tokens = syuzhet::get_tokens(txt_col[i], pattern = "\\W")
if(dimension == 'sentiment'){
text.scored = get_sentiment(text.tokens, method = 'custom', lexicon = sent_lexicon)
} else if(dimension == 'concreteness'){
text.scored = get_sentiment(text.tokens, method = 'custom', lexicon = concr_lexicon)
}
text.scored_binned = get_dct_transform(text.scored
, x_reverse_len=bins
, scale_range=T)
sentiment_rolled_rescaled = rescale_x_2(text.scored_binned)
empty_matrix[, i] = sentiment_rolled_rescaled$z
}
if(transposing == T){
final_df = as.data.frame(t(empty_matrix))
row.names(final_df) = txt_id_col
} else if(transposing == F){
final_df = as.data.frame(empty_matrix)
colnames(final_df) = txt_id_col
}
t2 = Sys.time()
print(t2-t1)
setwd(currentwd)
return(final_df)
}
data = data.frame('text' = character(2)
, 'text_id' = character(2))
data$text = c('this is a super, great positive sentence and I just love doing this'
, 'and now a bad, bad, and ugly example which I really hate')
data$text_id = c('text1', 'text2')
my_sentiment_analysis = get_narrative_dim(txt_input_col = data$text
, txt_id_col = data$text_id
, dimension = 'sentiment'
, stemming = F
, transposing = F)
plot(1:100
, my_sentiment_analysis$text1
, type ="h"
, col = "red")
my_concreteness_analysis = get_narrative_dim(txt_input_col = data$text
, txt_id_col = data$text_id
, dimension = 'concreteness'
, stemming = F
, transposing = F)
plot(1:100
, my_concreteness_analysis$text1
, type ="h"
, col = "red")
###########################################################
### TERR CLAIMS PROJECT
### ANALYSIS
### B Kleinberg, I van der Vegt, M Zaneva, B Verschuere
###########################################################
#clear wd
rm(list = ls())
#deps
setwd("/Users/bennettkleinberg/GitHub/r_helper_functions/")
source('./txt_df_from_dir.R')
source('./spacy_ner_r.R')
source('./spacy_pos_r.R')
source('./linguistic_category_model.R')
source('./calculate_concreteness.R')
source('./get_single_readability.R')
source('./sent_pipe_syuzhet_plus.R')
source('./toNumeric.R')
source('./ds_between_CI.R')
source('./dz_within_CI.R')
###########################################################
### TERR CLAIMS PROJECT
### ANALYSIS
### B Kleinberg, I van der Vegt, M Zaneva, B Verschuere
###########################################################
#clear wd
rm(list = ls())
#deps
setwd("/Users/bennettkleinberg/GitHub/r_helper_functions/")
source('./txt_df_from_dir.R')
source('./spacy_ner_r.R')
source('./spacy_pos_r.R')
source('./linguistic_category_model.R')
source('./calculate_concreteness.R')
source('./get_single_readability.R')
source('./get_narrative_dim.R')
source('./toNumeric.R')
source('./ds_between_CI.R')
source('./dz_within_CI.R')
a = get_narrative_dim(txt_input_col = data$text[1:20]
, txt_id_col = data$text_id[1:20]
, dimension = 'sentiment'
, stemming = F
, transposing = F)
###########################################################
### TERR CLAIMS PROJECT
### ANALYSIS
### B Kleinberg, I van der Vegt, M Zaneva, B Verschuere
###########################################################
#clear wd
rm(list = ls())
#deps
setwd("/Users/bennettkleinberg/GitHub/r_helper_functions/")
source('./txt_df_from_dir.R')
source('./spacy_ner_r.R')
source('./spacy_pos_r.R')
source('./linguistic_category_model.R')
source('./calculate_concreteness.R')
source('./get_single_readability.R')
source('./get_narrative_dim.R')
source('./toNumeric.R')
source('./ds_between_CI.R')
source('./dz_within_CI.R')
#packages
library(tidyr)
library(ggplot2)
library(Hmisc)
library(caret)
library(e1071)
library(pROC)
library(ez)
#library(quanteda)
library(MLmetrics)
#load data
setwd('/Users/bennettkleinberg/Documents/Research/CBDMI_Schiphol/terr_claims/data')
load('data_terr_claims.RData')
#merge with country annotation
country = read.csv('terr_claims_with_region.csv')
country = country[, c(3, 4)]
names(country) = c('statement_id', 'country')
country = na.omit(country)
country$country = as.factor(country$country)
levels(country$country) = c('ME', 'WEST', 'ASIA', 'AFRICA')
country$Filename = paste(as.character(country$statement_id), ".txt", sep="")
#merge with dates
dates = read.csv('terr_claims_with_date.csv')
dates = dates[, c(2, 11)]
dates$publication_date = as.Date(as.character(dates$publication_date)
, format = "%d/%m/%Y")
#####PART 2#####
#####Preparation#####
##regex for magazine, issue number
data$magazine = as.factor(substr(as.character(data$Filename), 1, 1))
data$issue = str_extract(as.character(data$Filename), pattern = '[[:digit:]]{1,2}')
data$statement_id = str_extract(gsub(pattern = '[[:upper:]]_[[:digit:]]{1,2}', "", as.character(data$Filename)), pattern = '[[:digit:]]{1,2}')
levels(data$magazine) = c('dabiq', 'rumiyah')
#merge
data = merge(data, country, by='Filename')
data = merge(data, dates, by='Filename')
#descriptive sample statistics
tapply(data$nwords, data$magazine, mean)
tapply(data$nwords, data$magazine, sd)
mean(data$nwords)
sd(data$nwords)
prop.table(table(data$country, data$magazine), 2)*100
prop.table(table(data$country))*100
table(data$magazine)
##set coding criterion score for veracity
minimum_source_agreement = 1
data$veracity = ifelse(data$isis_sum >= minimum_source_agreement, 1, 0)
prop.table(table(data$veracity))*100
prop.table(table(data$veracity, data$magazine), 2)*100
#set vars
data$how = data$pos_adj_rel + data$pos_adv_rel
data$why = data$cause.x/100
data$negativity = data$negemo.x - data$posemo.x
data$inf_language = data$pos_noun_rel + data$pos_adj_rel + data$prep.x/100 + data$pos_sconj_rel + data$pos_cconj_rel
data$imag_language = data$pos_verb_rel + data$pos_adv_rel + data$pos_pron_rel
data$person_index_weighted = (1*data$ppron.x + 2*data$social.x + 3*data$ner_person_nn)/(data$ppron.x + data$social.x + data$ner_person_nn)
data$perspective = (data$i.x + data$we.x) - (data$you.x + data$they.x)
data$first_ppron = data$i.x + data$we.x
data$third_ppron = data$shehe.x + data$they.x
data$personal_concerns = data$work.x + data$leisure.x + data$home.x + data$money.x + data$relig.x + data$death.x
##set id for merge later
data$text_id = paste('text', row.names(data), sep="")
data_min = data[, c('veracity', 'text_id')]
#deps
setwd("/Users/bennettkleinberg/GitHub/r_helper_functions/")
a = get_narrative_dim(txt_input_col = data$text[1:20]
, txt_id_col = data$text_id[1:20]
, dimension = 'sentiment'
, stemming = F
, transposing = F)
